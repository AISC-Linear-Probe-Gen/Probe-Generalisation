{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02899f49",
   "metadata": {},
   "source": [
    "# Standalone Probe Experiment, Using Existing Dataset\n",
    "You can just collapse the headings and run all the cells. It will load the file if its already computed instead of rerunning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378f6470",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64752e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from huggingface_hub import hf_hub_download\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6241d210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File I/O configuration\n",
    "OUTPUT_DIR = \"experiment_data\"  # Directory to save intermediate results\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "splits_file = \"splits.json\"\n",
    "activations_file = \"activations.pt\"\n",
    "splits_filepath = os.path.join(OUTPUT_DIR, splits_file)\n",
    "activations_filepath = os.path.join(OUTPUT_DIR, activations_file)\n",
    "\n",
    "# Set random seed\n",
    "PROBE_SEED = 42\n",
    "torch.manual_seed(PROBE_SEED)\n",
    "np.random.seed(PROBE_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3598527",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7296bfcd",
   "metadata": {},
   "source": [
    "### Define Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ebc1239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _download_labels_from_hf(repo_id: str, labels_filepath: str, output_path: str):\n",
    "    \"\"\"Download labels file from HuggingFace to a specific output path (flattened, no subdirectories).\"\"\"\n",
    "    # Download to temp directory first (preserves HF directory structure)\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        downloaded_path = hf_hub_download(\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"dataset\",\n",
    "            filename=labels_filepath,\n",
    "            local_dir=tmpdir,\n",
    "            token=os.environ.get(\"HF_TOKEN\")\n",
    "        )\n",
    "        # Move the file to the desired output path (flattened)\n",
    "        shutil.move(downloaded_path, output_path)\n",
    "\n",
    "\n",
    "def _load_labels_from_local_jsonl(labels_filename: str, verbose: bool = False) -> tuple[torch.Tensor, List[Dict]]:\n",
    "    \"\"\"Load labels and full data from local JSONL file.\"\"\"\n",
    "    labels_list = []\n",
    "    data_rows = []\n",
    "    with open(labels_filename, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            data_rows.append(data)\n",
    "            # Use 'labels' field directly (should be \"positive\" or \"negative\")\n",
    "            if data.get(\"labels\") == \"positive\":\n",
    "                labels_list.append(1.0)\n",
    "            elif data.get(\"labels\") == \"negative\":\n",
    "                labels_list.append(0.0)\n",
    "            else:\n",
    "                # Fallback to scale_labels thresholding if labels field not present\n",
    "                if data.get(\"scale_labels\", 5) <= 5:\n",
    "                    labels_list.append(1.0)\n",
    "                else:\n",
    "                    labels_list.append(0.0)\n",
    "    labels_tensor = torch.tensor(labels_list)\n",
    "    if verbose:\n",
    "        print(\"Loaded labels\")\n",
    "    return labels_tensor, data_rows\n",
    "\n",
    "\n",
    "def _load_activations_from_hf(repo_id: str, filename: str, verbose: bool = False):\n",
    "    \"\"\"Load activations from HuggingFace dataset.\"\"\"\n",
    "    # Load activations\n",
    "    file_path = hf_hub_download(\n",
    "        repo_id=repo_id,\n",
    "        filename=filename,\n",
    "        repo_type=\"dataset\",\n",
    "    )\n",
    "    df = joblib.load(file_path)\n",
    "\n",
    "    # Extract all activations\n",
    "    all_activations = []\n",
    "    for i in range(len(df)):\n",
    "        all_activations.append(df.loc[i][\"activations\"])\n",
    "    activations_tensor = pad_sequence(all_activations, batch_first=True, padding_value=0.0).to(torch.float32)\n",
    "    if verbose:\n",
    "        print(f\"Loaded activations with shape {activations_tensor.shape}\")\n",
    "\n",
    "    max_len = activations_tensor.shape[1]\n",
    "    masks = []\n",
    "    for tensor in all_activations:\n",
    "        current_len = tensor.shape[0]\n",
    "        mask = torch.ones(1, current_len)\n",
    "        if current_len < max_len:\n",
    "            padding_mask = torch.zeros(1, max_len - current_len)\n",
    "            mask = torch.cat([mask, padding_mask], dim=1)\n",
    "        masks.append(mask)\n",
    "    attention_mask = torch.cat(masks, dim=0)\n",
    "    if verbose:\n",
    "        print(f\"Calculated attention mask with shape {attention_mask.shape}\")\n",
    "\n",
    "    return activations_tensor, attention_mask\n",
    "\n",
    "\n",
    "def load_hf_activations_at_layer(\n",
    "    behaviour: str,\n",
    "    datasource: str,\n",
    "    activations_model: str = \"llama_3b\",\n",
    "    response_model: str = \"llama_3b\",\n",
    "    generation_method: str = \"on_policy\",\n",
    "    mode: str = \"train\",\n",
    "    layer: int = 12,\n",
    "    and_labels: bool = False,\n",
    "    verbose: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads activations for a specified layer and ground truth labels from Huggingface.\n",
    "\n",
    "    Args:\n",
    "        behaviour (str): Behaviour name.\n",
    "        datasource (str): Datasource name.\n",
    "        activations_model (str): Activations model name.\n",
    "        response_model (str): Response model name.\n",
    "        generation_method (str): \"on_policy\", \"prompted\", \"incentivised\", \"off_policy\".\n",
    "        mode (str): \"train\" or \"test\".\n",
    "        layer (int): Model layer we should get the activations from.\n",
    "        and_labels (bool): Whether to load labels.\n",
    "        verbose (bool): Should the function output to console.\n",
    "\n",
    "    Returns:\n",
    "        activations_tensor (tensor): tensor of activations of shape [batch_size, seq_len, dim].\n",
    "        attention_mask (tensor): tensor stating which tokens are real (1) or padded (0) of shape [batch_size, seq_len]\n",
    "        labels_tensor (tensor): tensor of ground truth labels of shape [batch_size].\n",
    "    \"\"\"\n",
    "    if datasource == \"trading\":\n",
    "        mode = \"3.5k\"\n",
    "\n",
    "    repo_id = HF_REPO_ID_TEMPLATE.format(behaviour=behaviour)\n",
    "    filepath = f\"{datasource}/{activations_model}/{response_model}_{generation_method}_{mode}_layer_{layer}.pkl\"\n",
    "\n",
    "    try:\n",
    "        activations_tensor, attention_mask = _load_activations_from_hf(repo_id, filepath, verbose)\n",
    "    except Exception as e:\n",
    "        # Try loading with on_policy in the name instead\n",
    "        if generation_method == \"off_policy\":\n",
    "            real_generation_method = \"incentivised\" if behaviour in [\"deception\", \"sandbagging\"] else \"on_policy\"\n",
    "            filepath = filepath.replace(\"off_policy\", real_generation_method)\n",
    "            activations_tensor, attention_mask = _load_activations_from_hf(repo_id, filepath, verbose)\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "    if and_labels:\n",
    "        generation_method_for_labels = generation_method.replace(\"_included\", \"\")\n",
    "        # Use just the filename, not the full path structure (flattened)\n",
    "        labels_filename = f\"{datasource}_{response_model}_{generation_method_for_labels}_{mode}.jsonl\"\n",
    "        labels_fullpath = os.path.join(OUTPUT_DIR, labels_filename)\n",
    "        \n",
    "        # For downloading, we still need the original path structure from HF\n",
    "        labels_filepath_hf = f\"{datasource}/{response_model}_{generation_method_for_labels}_{mode}.jsonl\"\n",
    "\n",
    "        try:\n",
    "            if not os.path.exists(labels_fullpath):\n",
    "                _download_labels_from_hf(repo_id, labels_filepath_hf, labels_fullpath)\n",
    "            labels_tensor, data_rows = _load_labels_from_local_jsonl(labels_fullpath, verbose)\n",
    "        except Exception as e:\n",
    "            # Try loading with on_policy in the name instead\n",
    "            if generation_method == \"off_policy\":\n",
    "                real_generation_method = \"incentivised\" if behaviour in [\"deception\", \"sandbagging\"] else \"on_policy\"\n",
    "                labels_filename = labels_filename.replace(\"off_policy\", real_generation_method)\n",
    "                labels_filepath_hf = labels_filepath_hf.replace(\"off_policy\", real_generation_method)\n",
    "                labels_fullpath = os.path.join(OUTPUT_DIR, labels_filename)\n",
    "                if not os.path.exists(labels_fullpath):\n",
    "                    _download_labels_from_hf(repo_id, labels_filepath_hf, labels_fullpath)\n",
    "                labels_tensor, data_rows = _load_labels_from_local_jsonl(labels_fullpath, verbose)\n",
    "            else:\n",
    "                raise e\n",
    "        return activations_tensor, attention_mask, labels_tensor, data_rows\n",
    "\n",
    "    else:\n",
    "        return activations_tensor, attention_mask, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df0cab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pool_activations(activations: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Mean pool activations across sequence length, masking padding tokens.\"\"\"\n",
    "    mask = attention_mask.unsqueeze(-1).float()\n",
    "    masked_activations = activations * mask\n",
    "    pooled = masked_activations.sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "    return pooled\n",
    "\n",
    "\n",
    "def create_activation_datasets(activations_tensor, labels_tensor, data_rows: List[Dict], splits=[3500, 500, 0], verbose=False):\n",
    "    \"\"\"\n",
    "    Create datasets from pre-aggregated activations.\n",
    "\n",
    "    Args:\n",
    "        activations_tensor (tensor): tensor of pre-aggregated activations of shape [batch_size, dim]\n",
    "        labels_tensor (tensor): tensor of ground truth labels of shape [batch_size]\n",
    "        data_rows (list): List of dicts with full data from JSONL (input, output, labels, scale_labels, etc.)\n",
    "        splits (list): [train_size, val_size, test_size]\n",
    "        verbose (bool): Should the function output to console.\n",
    "\n",
    "    Returns:\n",
    "        splits_dict (dict): {'train': [{'input': ..., 'output': ..., 'scale_labels': ..., 'label': ...}, ...], ...}\n",
    "        activations_dict (dict): {'train': activations_tensor, 'val': activations_tensor, 'test': activations_tensor}\n",
    "    \"\"\"\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    if len(splits) != 3:\n",
    "        raise ValueError(\"Splits must be a list of 3 numbers [train_size, val_size, test_size]\")\n",
    "\n",
    "    if sum(splits) > labels_tensor.shape[0]:\n",
    "        if (sum(splits) - (labels_tensor.shape[0])) > 500:\n",
    "            raise ValueError(\"Splits must sum to less than or equal to number of samples, within a margin of 500\")\n",
    "\n",
    "        # Keep the val and test sizes the same but reduce the train size\n",
    "        val_test_size = splits[1] + splits[2]\n",
    "        train_size = labels_tensor.shape[0] - val_test_size\n",
    "        print(f\"Do not have {splits[0]} training samples, using {train_size} instead\")\n",
    "        splits[0] = train_size\n",
    "\n",
    "    # Get indices for each label and subsample both classes to same size\n",
    "    label_0_indices = (labels_tensor == 0.0).nonzero(as_tuple=True)[0]\n",
    "    label_1_indices = (labels_tensor == 1.0).nonzero(as_tuple=True)[0]\n",
    "    min_class_count = min(len(label_0_indices), len(label_1_indices))\n",
    "    label_0_indices = label_0_indices[:min_class_count]\n",
    "    label_1_indices = label_1_indices[:min_class_count]\n",
    "\n",
    "    # Compute split sizes (divided by 2 because we have two classes)\n",
    "    n_train = splits[0] // 2\n",
    "    n_val = splits[1] // 2\n",
    "    n_test = splits[2] // 2\n",
    "\n",
    "    # Split label 0s\n",
    "    train_0 = label_0_indices[:n_train]\n",
    "    val_0 = label_0_indices[n_train:n_train + n_val]\n",
    "    test_0 = label_0_indices[n_train + n_val:n_train + n_val + n_test]\n",
    "\n",
    "    # Split label 1s\n",
    "    train_1 = label_1_indices[:n_train]\n",
    "    val_1 = label_1_indices[n_train:n_train + n_val]\n",
    "    test_1 = label_1_indices[n_train + n_val:n_train + n_val + n_test]\n",
    "\n",
    "    # Concatenate splits and shuffle within each\n",
    "    def get_split(indices_0, indices_1):\n",
    "        indices = torch.cat([indices_0, indices_1])\n",
    "        indices = indices[torch.randperm(len(indices))]\n",
    "        return indices\n",
    "\n",
    "    train_indices = get_split(train_0, train_1)\n",
    "    val_indices = get_split(val_0, val_1)\n",
    "    test_indices = get_split(test_0, test_1)\n",
    "\n",
    "    # Create splits dict with only essential fields\n",
    "    def create_split_data(indices):\n",
    "        split_data = []\n",
    "        for idx in indices:\n",
    "            idx_int = idx.item()\n",
    "            row = data_rows[idx_int]\n",
    "            \n",
    "            # Extract input and output from inputs field\n",
    "            input_content = None\n",
    "            output_content = None\n",
    "            \n",
    "            if 'inputs' in row:\n",
    "                # inputs might be a JSON string or a list\n",
    "                inputs_data = row['inputs']\n",
    "                if isinstance(inputs_data, str):\n",
    "                    inputs_data = json.loads(inputs_data)\n",
    "                \n",
    "                if isinstance(inputs_data, list) and len(inputs_data) >= 2:\n",
    "                    input_content = inputs_data[0].get('content', '')\n",
    "                    output_content = inputs_data[1].get('content', '')\n",
    "            \n",
    "            # Fallback to existing fields if inputs parsing failed\n",
    "            if input_content is None:\n",
    "                input_content = row.get('input', row.get('input_formatted', ''))\n",
    "            if output_content is None:\n",
    "                output_content = row.get('output', row.get('model_outputs', ''))\n",
    "            \n",
    "            # Extract label\n",
    "            label = row.get('labels', '')\n",
    "            if not label:\n",
    "                # Fallback: derive from scale_labels\n",
    "                if row.get('scale_labels', 5) <= 5:\n",
    "                    label = 'positive'\n",
    "                else:\n",
    "                    label = 'negative'\n",
    "            \n",
    "            # Create simplified data entry with only required fields\n",
    "            simplified_row = {\n",
    "                'input': input_content,\n",
    "                'output': output_content,\n",
    "                'scale_labels': row.get('scale_labels'),\n",
    "                'label': label\n",
    "            }\n",
    "            split_data.append(simplified_row)\n",
    "        return split_data\n",
    "    \n",
    "    splits_dict = {\n",
    "        'train': create_split_data(train_indices),\n",
    "        'val': create_split_data(val_indices),\n",
    "        'test': create_split_data(test_indices),\n",
    "    }\n",
    "\n",
    "    # Create activations dict\n",
    "    activations_dict = {\n",
    "        'train': activations_tensor[train_indices],\n",
    "        'val': activations_tensor[val_indices],\n",
    "        'test': activations_tensor[test_indices],\n",
    "    }\n",
    "\n",
    "    # Output balance\n",
    "    if verbose:\n",
    "        train_labels = labels_tensor[train_indices]\n",
    "        val_labels = labels_tensor[val_indices]\n",
    "        test_labels = labels_tensor[test_indices]\n",
    "        print(f\"Train: {train_labels.shape[0]} samples, {train_labels.sum()} positives\")\n",
    "        print(f\"Val:   {val_labels.shape[0]} samples, {val_labels.sum()} positives\")\n",
    "        print(f\"Test:  {test_labels.shape[0]} samples, {test_labels.sum()} positives\")\n",
    "\n",
    "    return splits_dict, activations_dict\n",
    "\n",
    "\n",
    "def save_splits(splits: Dict[str, List[Dict]], filename: str = \"splits.json\"):\n",
    "    \"\"\"Save splits to JSON file.\"\"\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(splits, f, indent=2)\n",
    "    print(f\"Saved splits to {filepath}\")\n",
    "\n",
    "\n",
    "def load_splits(filename: str = \"splits.json\") -> Dict[str, List[Dict]]:\n",
    "    \"\"\"Load splits from JSON file.\"\"\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    with open(filepath, 'r') as f:\n",
    "        splits = json.load(f)\n",
    "    print(f\"Loaded splits from {filepath}\")\n",
    "    return splits\n",
    "\n",
    "\n",
    "def save_activations(activations: Dict[str, torch.Tensor], filename: str = \"activations.pt\"):\n",
    "    \"\"\"Save activations to PyTorch file.\"\"\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    torch.save(activations, filepath)\n",
    "    print(f\"Saved activations to {filepath}\")\n",
    "\n",
    "\n",
    "def load_activations(filename: str = \"activations.pt\") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Load activations from PyTorch file.\"\"\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    activations = torch.load(filepath)\n",
    "    print(f\"Loaded activations from {filepath}\")\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c9942f",
   "metadata": {},
   "source": [
    "### Load Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15e92dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_REPO_ID_TEMPLATE = \"lasrprobegen/{behaviour}-activations\"\n",
    "\n",
    "# Training dataset configuration\n",
    "# Check behaviours and datasources and models in src/probe_gen/config.py\n",
    "BEHAVIOUR = \"sycophancy\" \n",
    "DATASOURCE = \"multichoice\" \n",
    "RESPONSE_MODEL = \"llama_3b\"\n",
    "ACTIVATIONS_MODEL = \"llama_3b\" \n",
    "LAYER = 12 # (every 3rd layer is on HF)\n",
    "\n",
    "SPLIT_VAL = 3500 \n",
    "SPLIT_VAL_SIZE = 500\n",
    "\n",
    "# Adjust split size based on behaviour/datasource (matching TrainProbe.ipynb logic)\n",
    "split_val = SPLIT_VAL\n",
    "if BEHAVIOUR in [\"deception\", \"sandbagging\"]:\n",
    "    split_val = 2500\n",
    "if DATASOURCE == \"shakespeare\":\n",
    "    split_val = 3000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22d04044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading splits and activations from files...\n",
      "Loaded splits from experiment_data/splits.json\n",
      "Loaded activations from experiment_data/activations.pt\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(splits_filepath) and os.path.exists(activations_filepath):\n",
    "    print(\"Loading splits and activations from files...\")\n",
    "    splits = load_splits(splits_file)\n",
    "    activations = load_activations(activations_file)\n",
    "    \n",
    "    # Extract labels from splits.json (derive binary from label field)\n",
    "    train_labels = torch.tensor([1.0 if item['label'] == 'positive' else 0.0 for item in splits['train']], dtype=torch.float32)\n",
    "    val_labels = torch.tensor([1.0 if item['label'] == 'positive' else 0.0 for item in splits['val']], dtype=torch.float32)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = {'X': activations['train'], 'y': train_labels}\n",
    "    val_dataset = {'X': activations['val'], 'y': val_labels}\n",
    "else:\n",
    "    # Load training activations and labels (mode=\"train\")\n",
    "    activations_tensor, attention_mask, labels_tensor, data_rows = load_hf_activations_at_layer(\n",
    "        BEHAVIOUR,\n",
    "        DATASOURCE,\n",
    "        ACTIVATIONS_MODEL,\n",
    "        RESPONSE_MODEL,\n",
    "        \"on_policy\",  # Always use \"on_policy\" generation method\n",
    "        \"train\",  # Always use \"train\" mode for training data\n",
    "        LAYER,\n",
    "        and_labels=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Apply mean pooling (before splitting)\n",
    "    print(\"\\nMean pooling activations...\")\n",
    "    activations_tensor = mean_pool_activations(activations_tensor, attention_mask)\n",
    "    print(f\"Pooled activations shape: {activations_tensor.shape}\")\n",
    "    \n",
    "    # Create train/val splits (no test split - test is loaded separately)\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Creating train/val splits\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    splits, activations = create_activation_datasets(\n",
    "        activations_tensor,\n",
    "        labels_tensor,\n",
    "        data_rows,\n",
    "        splits=[split_val, SPLIT_VAL_SIZE, 0],  # No test split\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Save splits (JSON) and activations (PyTorch) separately\n",
    "    save_splits(splits, splits_file)\n",
    "    save_activations(activations, activations_file)\n",
    "    \n",
    "    # Extract labels from splits.json (derive binary from label field)\n",
    "    train_labels = torch.tensor([1.0 if item['label'] == 'positive' else 0.0 for item in splits['train']], dtype=torch.float32)\n",
    "    val_labels = torch.tensor([1.0 if item['label'] == 'positive' else 0.0 for item in splits['val']], dtype=torch.float32)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = {'X': activations['train'], 'y': train_labels}\n",
    "    val_dataset = {'X': activations['val'], 'y': val_labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8e3c69",
   "metadata": {},
   "source": [
    "### Load Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4c9cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset configuration\n",
    "# Set to any value to None to use same as training, or specify different values\n",
    "TEST_BEHAVIOUR = None\n",
    "TEST_DATASOURCE = \"arguments\" # \"arguments\" for OOD, None for ID\n",
    "TEST_RESPONSE_MODEL = None\n",
    "\n",
    "TEST_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bffa450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded activations with shape torch.Size([1000, 292, 3072])\n",
      "Calculated attention mask with shape torch.Size([1000, 292])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e340045de1c24b4bacaf655a8a7427c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama_3b_on_policy_test.jsonl: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded labels\n",
      "\n",
      "Mean pooling test activations...\n",
      "Pooled test activations shape: torch.Size([1000, 3072])\n",
      "Train: 0 samples, 0.0 positives\n",
      "Val:   0 samples, 0.0 positives\n",
      "Test:  1000 samples, 500.0 positives\n"
     ]
    }
   ],
   "source": [
    "# Determine test set parameters (use training params if None)\n",
    "test_behaviour = TEST_BEHAVIOUR if TEST_BEHAVIOUR is not None else BEHAVIOUR\n",
    "test_datasource = TEST_DATASOURCE if TEST_DATASOURCE is not None else DATASOURCE\n",
    "# Always use same activations model and layer as training\n",
    "test_activations_model = ACTIVATIONS_MODEL\n",
    "test_response_model = TEST_RESPONSE_MODEL if TEST_RESPONSE_MODEL is not None else RESPONSE_MODEL\n",
    "test_layer = LAYER\n",
    "\n",
    "# Load test activations and labels (mode=\"test\", always \"on_policy\")\n",
    "test_activations_tensor, test_attention_mask, test_labels_tensor, test_data_rows = load_hf_activations_at_layer(\n",
    "    test_behaviour,\n",
    "    test_datasource,\n",
    "    test_activations_model,\n",
    "    test_response_model,\n",
    "    \"on_policy\",  # Always use \"on_policy\" generation method\n",
    "    \"test\",  # Always use \"test\" mode for test data\n",
    "    test_layer,\n",
    "    and_labels=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Apply mean pooling to test data\n",
    "print(\"\\nMean pooling test activations...\")\n",
    "test_activations_tensor = mean_pool_activations(test_activations_tensor, test_attention_mask)\n",
    "print(f\"Pooled test activations shape: {test_activations_tensor.shape}\")\n",
    "\n",
    "# Create test dataset (splits=[0, 0, test_size] means all data goes to test)\n",
    "test_splits, test_activations = create_activation_datasets(\n",
    "    test_activations_tensor,\n",
    "    test_labels_tensor,\n",
    "    test_data_rows,\n",
    "    splits=[0, 0, TEST_SIZE],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Extract labels from splits (derive binary from label field)\n",
    "test_labels = torch.tensor([1.0 if item['label'] == 'positive' else 0.0 for item in test_splits['test']], dtype=torch.float32)\n",
    "test_dataset = {'X': test_activations['test'], 'y': test_labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d54902b",
   "metadata": {},
   "source": [
    "## Probe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91676666",
   "metadata": {},
   "source": [
    "### Create and Train Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7aef5898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probe hyperparameters\n",
    "PROBE_TRAINING_METHOD = \"sklearn\"  # \"adam\" or \"sklearn\"\n",
    "PROBE_LR = 0.001\n",
    "PROBE_WEIGHT_DECAY = 0.01\n",
    "PROBE_NORMALIZE = True\n",
    "PROBE_USE_BIAS = True\n",
    "PROBE_EPOCHS = 100\n",
    "PROBE_PATIENCE = 10\n",
    "PROBE_C = 1.0  # For sklearn logistic regression (inverse of regularization strength)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e142af",
   "metadata": {},
   "source": [
    "#### Option 1: PyTorch linear probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02cf8641",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchLinearProbe(nn.Module):\n",
    "    \"\"\"PyTorch linear probe for binary classification with mean pooling aggregation.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, normalize: bool = True, use_bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1, bias=use_bias)\n",
    "        self.normalize = normalize\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "    \n",
    "    def fit_normalization(self, activations: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        \"\"\"Fit normalization parameters on pooled activations.\"\"\"\n",
    "        if self.normalize:\n",
    "            # Pool first, then compute stats\n",
    "            pooled = mean_pool_activations(activations, attention_mask)\n",
    "            self.mean = pooled.mean(dim=0, keepdim=True)\n",
    "            self.std = pooled.std(dim=0, keepdim=True)\n",
    "            self.std = torch.where(self.std == 0, torch.ones_like(self.std), self.std)\n",
    "    \n",
    "    def normalize_input(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Normalize input.\"\"\"\n",
    "        if self.normalize and self.mean is not None:\n",
    "            return (X - self.mean.to(X.device)) / self.std.to(X.device)\n",
    "        return X\n",
    "    \n",
    "    def forward(self, activations: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass: pool, normalize, then linear layer.\"\"\"\n",
    "        # Pool activations: [batch, seq_len, hidden_dim] -> [batch, hidden_dim]\n",
    "        pooled = mean_pool_activations(activations, attention_mask)\n",
    "        # Normalize\n",
    "        pooled_norm = self.normalize_input(pooled)\n",
    "        # Linear layer\n",
    "        return self.linear(pooled_norm).squeeze(-1)\n",
    "\n",
    "\n",
    "def create_and_train_probe_adam(\n",
    "    train_dataset: Dict[str, torch.Tensor],\n",
    "    val_dataset: Optional[Dict[str, torch.Tensor]] = None,\n",
    ") -> TorchLinearProbe:\n",
    "    \"\"\"Create and train a PyTorch linear probe using Adam optimizer.\"\"\"\n",
    "    train_X = train_dataset['X']\n",
    "    train_y = train_dataset['y']\n",
    "    \n",
    "    # Check if activations are already pooled (2D) or need pooling (3D)\n",
    "    if len(train_X.shape) == 2:\n",
    "        # Already pooled: [batch, dim]\n",
    "        input_dim = train_X.shape[1]\n",
    "        use_pooling = False\n",
    "    else:\n",
    "        # Need pooling: [batch, seq_len, dim]\n",
    "        input_dim = train_X.shape[2]\n",
    "        use_pooling = True\n",
    "        # Create dummy masks if not provided (all ones)\n",
    "        train_masks = torch.ones(train_X.shape[0], train_X.shape[1])\n",
    "    \n",
    "    probe = TorchLinearProbe(input_dim, normalize=PROBE_NORMALIZE, use_bias=PROBE_USE_BIAS)\n",
    "    print(f\"Created TorchLinearProbe with input_dim={input_dim}\")\n",
    "    \n",
    "    # Move to device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    probe = probe.to(device)\n",
    "    train_X = train_X.to(device)\n",
    "    train_y = train_y.float().to(device)\n",
    "    \n",
    "    # Fit normalization\n",
    "    if use_pooling:\n",
    "        train_masks = train_masks.to(device)\n",
    "        probe.fit_normalization(train_X, train_masks)\n",
    "    else:\n",
    "        # For already pooled activations, fit on the pooled data directly\n",
    "        if PROBE_NORMALIZE:\n",
    "            probe.mean = train_X.mean(dim=0, keepdim=True)\n",
    "            probe.std = train_X.std(dim=0, keepdim=True)\n",
    "            probe.std = torch.where(probe.std == 0, torch.ones_like(probe.std), probe.std)\n",
    "    \n",
    "    # Setup training\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(probe.parameters(), lr=PROBE_LR, weight_decay=PROBE_WEIGHT_DECAY)\n",
    "    \n",
    "    if use_pooling:\n",
    "        train_tensor_dataset = TensorDataset(train_X, train_masks, train_y)\n",
    "    else:\n",
    "        train_tensor_dataset = TensorDataset(train_X, train_y)\n",
    "    train_loader = DataLoader(train_tensor_dataset, batch_size=128, shuffle=True)\n",
    "    \n",
    "    # Setup validation if available\n",
    "    val_loader = None\n",
    "    if val_dataset is not None:\n",
    "        val_X = val_dataset['X'].to(device)\n",
    "        val_y = val_dataset['y'].float().to(device)\n",
    "        if use_pooling:\n",
    "            val_masks = torch.ones(val_X.shape[0], val_X.shape[1]).to(device)\n",
    "            val_tensor_dataset = TensorDataset(val_X, val_masks, val_y)\n",
    "        else:\n",
    "            val_tensor_dataset = TensorDataset(val_X, val_y)\n",
    "        val_loader = DataLoader(val_tensor_dataset, batch_size=128, shuffle=False)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_state = None\n",
    "    \n",
    "    for epoch in range(PROBE_EPOCHS):\n",
    "        # Train\n",
    "        probe.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            if use_pooling:\n",
    "                acts_batch, masks_batch, y_batch = batch\n",
    "                logits = probe(acts_batch, masks_batch)\n",
    "            else:\n",
    "                acts_batch, y_batch = batch\n",
    "                # For already pooled, just pass through linear layer\n",
    "                acts_norm = probe.normalize_input(acts_batch)\n",
    "                logits = probe.linear(acts_norm).squeeze(-1)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validate\n",
    "        if val_loader is not None:\n",
    "            probe.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    if use_pooling:\n",
    "                        acts_batch, masks_batch, y_batch = batch\n",
    "                        logits = probe(acts_batch, masks_batch)\n",
    "                    else:\n",
    "                        acts_batch, y_batch = batch\n",
    "                        acts_norm = probe.normalize_input(acts_batch)\n",
    "                        logits = probe.linear(acts_norm).squeeze(-1)\n",
    "                    loss = criterion(logits, y_batch)\n",
    "                    val_loss += loss.item()\n",
    "            val_loss /= len(val_loader)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                best_state = probe.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= PROBE_PATIENCE:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "        else:\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}: train_loss={train_loss:.4f}\")\n",
    "    \n",
    "    # Load best model\n",
    "    if best_state is not None:\n",
    "        probe.load_state_dict(best_state)\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    return probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a034ba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROBE_TRAINING_METHOD == \"adam\":\n",
    "    probe = create_and_train_probe_adam(\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835ade7d",
   "metadata": {},
   "source": [
    "#### Option 2: scikit-learn logistic regression probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81da953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_train_probe_sklearn(\n",
    "    train_dataset: Dict[str, torch.Tensor],\n",
    "    val_dataset: Optional[Dict[str, torch.Tensor]] = None,\n",
    ") -> LogisticRegression:\n",
    "    \"\"\"Create and train a scikit-learn logistic regression probe.\"\"\"\n",
    "    train_X = train_dataset['X']\n",
    "    train_y = train_dataset['y']\n",
    "    \n",
    "    # Check if activations are already pooled (2D) or need pooling (3D)\n",
    "    if len(train_X.shape) == 3:\n",
    "        # Need pooling: [batch, seq_len, dim]\n",
    "        print(\"Mean pooling activations...\")\n",
    "        # Create dummy masks if not provided (all ones)\n",
    "        train_masks = torch.ones(train_X.shape[0], train_X.shape[1])\n",
    "        train_X = mean_pool_activations(train_X, train_masks)\n",
    "    \n",
    "    train_X = train_X.cpu().numpy()\n",
    "    train_y = train_y.cpu().numpy()\n",
    "    \n",
    "    if val_dataset is not None:\n",
    "        val_X = val_dataset['X']\n",
    "        val_y = val_dataset['y']\n",
    "        if len(val_X.shape) == 3:\n",
    "            val_masks = torch.ones(val_X.shape[0], val_X.shape[1])\n",
    "            val_X = mean_pool_activations(val_X, val_masks)\n",
    "        val_X = val_X.cpu().numpy()\n",
    "        val_y = val_y.cpu().numpy()\n",
    "    else:\n",
    "        val_X = None\n",
    "        val_y = None\n",
    "    \n",
    "    # Normalize if needed\n",
    "    if PROBE_NORMALIZE:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        train_X = scaler.fit_transform(train_X)\n",
    "        if val_X is not None:\n",
    "            val_X = scaler.transform(val_X)\n",
    "    else:\n",
    "        scaler = None\n",
    "    \n",
    "    # Train logistic regression\n",
    "    print(\"Fitting LogisticRegression...\")\n",
    "    clf = LogisticRegression(\n",
    "        C=PROBE_C,\n",
    "        fit_intercept=PROBE_USE_BIAS,\n",
    "        max_iter=1000,\n",
    "        random_state=PROBE_SEED,\n",
    "        solver='lbfgs',\n",
    "    )\n",
    "    clf.fit(train_X, train_y)\n",
    "    \n",
    "    # Store scaler for later use\n",
    "    clf.scaler = scaler\n",
    "    \n",
    "    # Print validation accuracy if available\n",
    "    if val_X is not None:\n",
    "        val_pred = clf.predict(val_X)\n",
    "        val_acc = accuracy_score(val_y, val_pred)\n",
    "        print(f\"Validation accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c5910cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LogisticRegression...\n",
      "Validation accuracy: 0.8240\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "if PROBE_TRAINING_METHOD == \"sklearn\":\n",
    "    probe = create_and_train_probe_sklearn(\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7966d2f1",
   "metadata": {},
   "source": [
    "### Evaluate Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61d9b6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_probe(\n",
    "    probe,\n",
    "    test_dataset: Dict[str, torch.Tensor],\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate the probe on test set.\"\"\"\n",
    "    test_X = test_dataset['X']\n",
    "    test_y = test_dataset['y']\n",
    "    \n",
    "    # Determine probe type by checking if it's a sklearn LogisticRegression\n",
    "    if isinstance(probe, LogisticRegression):\n",
    "        # sklearn probe\n",
    "        if len(test_X.shape) == 3:\n",
    "            # Need pooling\n",
    "            test_masks = torch.ones(test_X.shape[0], test_X.shape[1])\n",
    "            test_X = mean_pool_activations(test_X, test_masks)\n",
    "        \n",
    "        test_X = test_X.cpu().numpy()\n",
    "        \n",
    "        if hasattr(probe, 'scaler') and probe.scaler is not None:\n",
    "            test_X = probe.scaler.transform(test_X)\n",
    "        \n",
    "        probs = probe.predict_proba(test_X)[:, 1]\n",
    "        preds = probe.predict(test_X)\n",
    "        y_true = test_y.cpu().numpy()\n",
    "    else:\n",
    "        # PyTorch probe (or any other non-sklearn probe)\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        probe.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test_X = test_X.to(device)\n",
    "            test_y = test_y.to(device)\n",
    "            \n",
    "            if len(test_X.shape) == 3:\n",
    "                # Need pooling\n",
    "                test_masks = torch.ones(test_X.shape[0], test_X.shape[1]).to(device)\n",
    "                logits = probe(test_X, test_masks)\n",
    "            else:\n",
    "                # Already pooled\n",
    "                test_X_norm = probe.normalize_input(test_X)\n",
    "                logits = probe.linear(test_X_norm).squeeze(-1)\n",
    "            \n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "            y_true = test_y.cpu().numpy()\n",
    "    y_proba = probs\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, preds)\n",
    "    auroc = roc_auc_score(y_true, y_proba)\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "    target_fpr = 0.01\n",
    "    idx = np.argmax(fpr >= target_fpr)\n",
    "    tpr_at_1_fpr = tpr[idx] if idx < len(tpr) else 0.0\n",
    "    \n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'auroc': auroc,\n",
    "        'tpr_at_1_fpr': tpr_at_1_fpr,\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    for metric, value in results.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c89482d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "accuracy: 0.8310\n",
      "auroc: 0.9084\n",
      "tpr_at_1_fpr: 0.1640\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Evaluate probe on test set\n",
    "results = evaluate_probe(\n",
    "    probe,\n",
    "    test_dataset,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv)",
   "language": "python",
   "name": "uv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
